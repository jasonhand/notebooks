{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "UgHJ5WTvCeAl",
      "metadata": {
        "id": "UgHJ5WTvCeAl"
      },
      "source": [
        "#  Generative AI NASA Short Stories With the Gemini Pro Model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "FCZxXpOiR-SC",
      "metadata": {
        "id": "FCZxXpOiR-SC"
      },
      "source": [
        "## Environment Setup\n",
        "Install packages.\n",
        ">Secrets (API keys) are **NOT** used in this demo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Uu8xHNjWtjxKzC41WDAghpdA",
      "metadata": {
        "id": "Uu8xHNjWtjxKzC41WDAghpdA",
        "tags": []
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade google-cloud-aiplatform"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4TjV4bq4aIXKoGURUlYjX30W",
      "metadata": {
        "id": "4TjV4bq4aIXKoGURUlYjX30W",
        "tags": []
      },
      "source": [
        "## Imports & Variables\n",
        "Set import requirements and set `image_info_url` variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "8Q-lriV2yWBZ",
      "metadata": {
        "executionInfo": {
          "elapsed": 256,
          "status": "ok",
          "timestamp": 1705531504164,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": 420
        },
        "id": "8Q-lriV2yWBZ"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import requests\n",
        "\n",
        "# URL to fetch the image information\n",
        "image_info_url = \"https://us-central1-datadog-community.cloudfunctions.net/get-nasa-image\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LAsAj61BQlvR",
      "metadata": {
        "id": "LAsAj61BQlvR"
      },
      "source": [
        "## Fetch the NASA Image\n",
        "Call a cloud function `image_info_url` from above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "n4aeiK9lQi4t",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 783,
          "status": "ok",
          "timestamp": 1705531569240,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": 420
        },
        "id": "n4aeiK9lQi4t",
        "outputId": "7d5acff1-8c25-4a65-e46e-96d3e76d5981"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Title: Trifecta at Twilight\n",
            "URL: https://apod.nasa.gov/apod/image/2002/ISS_Moon_Mars_composite1024.jpg\n",
            "Description: On February 18, as civil twilight began in northern New Mexico skies, the International Space Station, a waning crescent Moon, and planet Mars for a moment shared this well-planned single field of view. From the photographer's location the sky had just begun to grow light, but the space station orbiting 400 kilometers above the Earth was already bathed in the morning sunlight. At 6:25am local time it took about a second to cross in front of the lunar disk moving right to left in the composited successive frames. At the time, Mars itself had already emerged from behind the Moon following its much anticipated lunar occultation. The yellowish glow of the Red Planet is still in the frame at the upper right, beyond the Moon's dark edge.\n"
          ]
        }
      ],
      "source": [
        "# Fetch the image URL or text\n",
        "response = requests.get(image_info_url)\n",
        "if response.status_code == 200:\n",
        "    # Print the text returned from the request\n",
        "    print(response.text)  # Displaying the text\n",
        "else:\n",
        "    print(\"Failed to retrieve the image information. Sorry\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RkfcpX_Q4es2",
      "metadata": {
        "id": "RkfcpX_Q4es2"
      },
      "source": [
        "## Set the Prompt & Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "OggdWlaFuaCL",
      "metadata": {
        "executionInfo": {
          "elapsed": 158,
          "status": "ok",
          "timestamp": 1705531686661,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": 420
        },
        "id": "OggdWlaFuaCL"
      },
      "outputs": [],
      "source": [
        "#Model name to use\n",
        "model_name = \"gemini-pro\"\n",
        "\n",
        "#Prompt to send to Vertex AI\n",
        "prompt = \"Write a children's short story based on the text provided. Include at least one fictional character as the hero. Include the original title and image URL at the beginning of the story. Do not provide words that violate Google's Responsible AI practices\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "z3-vQlk1urMU",
      "metadata": {
        "id": "z3-vQlk1urMU"
      },
      "source": [
        "## Fetch Image Info and Send to Model with Prompt\n",
        "Run the following code to import dependencies and make calls to get image url and send the prompt to the model endpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2dYopgXutqi",
      "metadata": {
        "id": "b2dYopgXutqi"
      },
      "outputs": [],
      "source": [
        "#Import requests\n",
        "import vertexai\n",
        "from vertexai.preview.generative_models import GenerativeModel, Part\n",
        "\n",
        "# Function to fetch image information from the URL as plain text\n",
        "def fetch_image_info(url):\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        return response.text  # Return the plain text response\n",
        "    else:\n",
        "        return \"Error: Could not fetch image information\"\n",
        "\n",
        "# Fetching the image information\n",
        "image_info = fetch_image_info(image_info_url)\n",
        "\n",
        "# Generate short story based on image_info\n",
        "def generate(model_name):\n",
        "    model = GenerativeModel(model_name)\n",
        "    full_prompt = prompt + \"\\n\" + image_info  # Combining the user prompt with the image information\n",
        "    responses = model.generate_content(\n",
        "        full_prompt,  # Using the combined prompt\n",
        "        generation_config={\n",
        "            \"max_output_tokens\": 2048,\n",
        "            \"temperature\": 0.98,\n",
        "            \"top_p\": 1\n",
        "        },\n",
        "        stream=True,\n",
        "    )\n",
        "    # Print the generated content\n",
        "    for response in responses:\n",
        "        print(response.candidates[0].content.parts[0].text)\n",
        "\n",
        "# Call the generate function with the user-defined model name\n",
        "generate(model_name)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
